{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Task Definition\n",
    "\n",
    "Use regression techniques to predict the value of the target variable, price, based independent feature variables such as vehicle year, mileage, etc available in the dataset. This will be accomplished using by applying exploratory data analysis, feature engineering, and model selection to the provided dataset to identify important features which are predictors of sale price. The assumption is that this data analysis is for a 'standard' used car dealership that does not deal with exoctic or extremely vintage vehicles. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn import set_config\n",
    "\n",
    "# Set config to display the diagram\n",
    "set_config(display='diagram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/vehicles.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume id is unique for all columns, remove it before checking for duplicates\n",
    "#Remove features that will not be usefull based on initial inspection: id, vin, model\n",
    "df.drop(columns = ['id'], inplace=True)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f'Total number of duplicate rows: {duplicate_count}')\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VIN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'VIN', 'price', and 'odometer' are the same\n",
    "df = df.drop_duplicates(subset=['VIN', 'price', 'odometer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove features that will not be usefull based on initial inspection: vin, model\n",
    "df.drop(columns = ['VIN', 'model'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the year column, there appears to be some outliers since the min year is 1900. Even if these are not outliers, they are not useful for our analysis for a used car dealership. \n",
    "#Same goes for the price column, where the min price is 0 and very high max price.\n",
    "#Remove rows where odometer is greater than 500000 miles or 0 miles (even new cars have greater than 0 miles)\n",
    "df = df[df['year'] >= 1980]\n",
    "df = df[(df['price'] >= 1000) & (df['price'] <= 150000)]\n",
    "df = df[(df['odometer'] > 0) & (df['odometer'] <= 500000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (15, 4))\n",
    "ax[0].hist(df['price'])\n",
    "ax[0].grid()\n",
    "ax[0].set_title('Original price')\n",
    "ax[0].set_xlabel('Price')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[1].hist(np.log1p(df['price']))\n",
    "ax[1].grid()\n",
    "ax[1].set_title('Logarithm of price');\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].set_xlabel('Log Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side plots of price and log price\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histogram of original price\n",
    "sns.histplot(df['price'], kde=True, ax=axes[0], bins=20, color='blue', edgecolor='black')\n",
    "axes[0].set_title('Histogram and KDE of Price')\n",
    "axes[0].set_xlabel('Price')\n",
    "axes[0].set_ylabel('Density')\n",
    "\n",
    "# Histogram of log-transformed price\n",
    "sns.histplot(np.log(df['price']), kde=True, ax=axes[1], bins=20, color='green', edgecolor='black')\n",
    "axes[1].set_title('Histogram and KDE of Log(Price)')\n",
    "axes[1].set_xlabel('Log(Price)')\n",
    "axes[1].set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price appears to have a logarithmetic distribution add log(price) to dataframe\n",
    "#df['log_price'] = np.log(df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for numeric features\n",
    "#numeric_features = ['year', 'odometer']\n",
    "#categorical_features = ['condition', 'drive', 'transmission', 'fuel', 'type']\n",
    "#for feature in numeric_features:\n",
    "#        for categorical_feature in categorical_features:\n",
    "#            plt.figure(figsize=(8, 6))\n",
    "#            sns.scatterplot(x=feature, y='price', hue=categorical_feature, data=df)\n",
    "#            plt.title(f'Scatter Plot of {feature} vs Price')\n",
    "#            plt.xlabel(feature)\n",
    "#            plt.ylabel('Price ($)')\n",
    "#            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a closer look at remaining categorical features. Decide which ones have ordinality and will be converted to numeric features, which ones are suitible for one hot encoding and which ones should be dropped due to large dimensionality and/or missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_status'].unique()\n",
    "df['title_status'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the vast majority of title_status values are 'clean' and the fact that most used car dealerships would not sell cars with other status values but would instead send them to an auction, the records with values other than clean will be removed and then the title_status column will be removed because it provides no value to linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title_status'] == 'clean']\n",
    "df.drop(columns = 'title_status', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find features with high percentage of missing values\n",
    "(df.isna().sum() / len(df)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Bar plot of missing values per feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns with more than 50% missing values - size is the only one\n",
    "df.drop(columns = 'size', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine the number of unqiue values for categorical features\n",
    "for col in df.select_dtypes(include=['object', 'category']):\n",
    "    print(f'{col}: # unique {len(df[col].unique())} % missing {round((df[col].isnull().sum()/len(df)) * 100)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create barplot of average price by state because state has 50 unique values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='state', y='price', data=df, estimator='mean', ci=None, palette='muted')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Average Price by State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Average Price ($)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots for categorical features - excluding state and region due to high dimensionality\n",
    "categorical_features = ['drive', 'type', 'transmission', 'fuel', 'paint_color']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=feature, y='price', data=df)\n",
    "    plt.title(f'Box Plot of {feature} vs Price')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store correlation values\n",
    "correlation_table = pd.DataFrame()\n",
    "\n",
    "#create a dataframes with desired categorical variables and examine their correlations with price\n",
    "categorical_features = ['manufacturer', 'condition', 'cylinders', 'fuel', 'transmission', 'drive', 'paint_color', 'type', 'state']\n",
    "for feature in categorical_features:\n",
    "    print(f\"Correlation of {feature} with price\")\n",
    "    categorical_df = pd.get_dummies(df[feature], dtype='int64')\n",
    "    categorical_df['price'] = df['price']\n",
    "    categorical_df.dropna(inplace=True)\n",
    "    corr_matrix = categorical_df.corr()\n",
    "\n",
    "    corr_with_price = corr_matrix['price'].drop('price').round(5)\n",
    "\n",
    "    # Calculate the count of records for each category\n",
    "    category_counts = categorical_df.drop(columns='price').sum()\n",
    "\n",
    "    print(corr_with_price)\n",
    "    print('\\n') \n",
    "\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Feature': feature,\n",
    "        'Category': corr_with_price.index,\n",
    "        'Correlation with Price': corr_with_price.values,\n",
    "        'Record Count': category_counts.values\n",
    "    })\n",
    "    # Concatenate the temporary DataFrame with the main correlation table\n",
    "    correlation_table = pd.concat([correlation_table, temp_df], ignore_index=True)\n",
    "\n",
    "    correlation_table.to_csv('categorical_correlation_report.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The region and state columns are not useful for our analysis due to high dimensionality. We will drop these columns.\n",
    "#There may be a correlation between the state and the price of the car, but we will not be able to use this information in a linear regression model.\n",
    "#The 'type' is being dropped because it has relatively high dimensionality and relatively high percentatge of missing values.\n",
    "df.drop(columns = ['region', 'type', 'state'], inplace=True)\n",
    "df.describe(include='all').round(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of cylinders is probably an important factor in the price of the vehicle, I want to keep the column even though 43% of the values are missing. In order to keep the feature it will be transformed to a numerical feature. For the missing cylinder values and the 'other' values, the average of the other values will be used.\n",
    "The condition feature will also be converted to a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the number of cylinders to a numeric value\n",
    "cylinder_map = {\n",
    "    '6 cylinders': 6,\n",
    "    '4 cylinders': 4,\n",
    "    '8 cylinders': 8,\n",
    "    '5 cylinders': 5,\n",
    "    '10 cylinders': 10,\n",
    "    '3 cylinders': 3,\n",
    "    '12 cylinders': 12,\n",
    "    'other': np.nan,   \n",
    "    'NaN': np.nan \n",
    "}\n",
    "\n",
    "df['cylinders_numeric'] = df['cylinders'].map(cylinder_map)\n",
    "average_num_cylinders = df['cylinders_numeric'].mean()\n",
    "df['cylinders_numeric'].fillna(average_num_cylinders, inplace=True)\n",
    "df.drop(columns='cylinders', inplace=True)\n",
    "df.rename(columns={'cylinders_numeric': 'cylinders'}, inplace=True)\n",
    "df['cylinders'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the condition to a numeric value\n",
    "\n",
    "#not interested in salvaged cars, drop them\n",
    "df = df[df['condition'] != 'salvage']\n",
    "\n",
    "condition_map = {\n",
    "    'new': 5,\n",
    "    'like new': 4,\n",
    "    'excellent': 3,\n",
    "    'good': 2,\n",
    "    'fair': 1,  \n",
    "    'NaN': np.nan  \n",
    "}\n",
    "\n",
    "\n",
    "df['condition'] = df['condition'].map(condition_map)\n",
    "average_condition = df['condition'].mean()\n",
    "df['condition'].fillna(average_condition, inplace=True)\n",
    "df['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove categorical features based on analysis\n",
    "#drop manufacturer because there are too many unique values and is not suitable for a linear regression model. It also has a low correlation\n",
    "#drop paint_color and type because there are too many missing values and as well as too many unique values\n",
    "#drop transmission, from scatter plot it is seen that the vast majority of values are automatic\n",
    "df.drop(columns=['manufacturer', 'paint_color', 'transmission'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['drive'].fillna('unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the type column, we will use the top 3 correlated values and assign all other values to the 'other' category\n",
    "#top_types = ['pickup', 'truck', 'sedan']\n",
    "#df['type'] = df['type'].apply(lambda x: x if x in top_types else 'other')\n",
    "\n",
    "#Apply one-hot encoding\n",
    "#df_encoded = pd.get_dummies(df, columns=['type'], dtype='int64')\n",
    "\n",
    "#df_encoded.drop(columns='type_other', inplace=True)\n",
    "#df = df_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop remaining missing values\n",
    "df.dropna(inplace=True)\n",
    "df.describe().round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at basic linear regression models for individual features to detect features with non-linear relationship to price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric_df = df.select_dtypes(include=['float64'])\n",
    "#numeric_df['price'] = df['price']\n",
    "#for col in numeric_df:\n",
    "#    if(col != 'price'):\n",
    "#        plt.figure(figsize=(10, 8))\n",
    "#        sns.regplot(x=col, y='price', data=numeric_df)\n",
    "#        plt.title(f'Price vs {col}')\n",
    "#        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=['float64'])\n",
    "numeric_df['price'] = df['price']\n",
    "correlation_matrix = numeric_df.corr()\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', cbar=True, annot_kws={\"size\": 12})\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix for all numeric features\n",
    "corr_matrix = numeric_df.corr()\n",
    "corr_matrix['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df.drop(columns='price')\n",
    "#y = np.log(df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convenience function provided by Jessica Cervi to calculate error metrics\n",
    "def error_metrics(y_train_true, y_train_pred, y_test_true, y_test_pred):\n",
    "    \n",
    "    errors = {}\n",
    "    \n",
    "    # Errors for train data\n",
    "    errors[\"Train_MAE\"] = mean_absolute_error(y_train_true, y_train_pred)\n",
    "    errors[\"Train_MSE\"] = mean_squared_error(y_train_true, y_train_pred)\n",
    "    errors[\"Train_RMSE\"] = np.sqrt(errors[\"Train_MSE\"])\n",
    "    errors[\"Train_R2_Score\"] = r2_score(y_train_true, y_train_pred)\n",
    "    \n",
    "    # Errors for test data\n",
    "    errors[\"Test_MAE\"] = mean_absolute_error(y_test_true, y_test_pred)\n",
    "    errors[\"Test_MSE\"] = mean_squared_error(y_test_true, y_test_pred)\n",
    "    errors[\"Test_RMSE\"] = np.sqrt(errors[\"Test_MSE\"])\n",
    "    errors[\"Test_R2_Score\"] = r2_score(y_test_true, y_test_pred)\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convenience function to assist performing simple cross-validation\n",
    "def get_errors_for_degree_k_model(k, x_train, y_train, x_test, y_test):\n",
    "    pipelined_model = Pipeline([\n",
    "        ('poly_features', PolynomialFeatures(degree=k, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('linearRegression', LinearRegression(fit_intercept=True))\n",
    "    ])\n",
    "    pipelined_model.fit(x_train, y_train)\n",
    "    y_train_pred = pipelined_model.predict(x_train)\n",
    "    y_test_pred = pipelined_model.predict(x_test)\n",
    "\n",
    "    return error_metrics(y_train, y_train_pred, y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple cross-validation \n",
    "train_mses = []\n",
    "test_mses = []\n",
    "numeric_df = df[['year', 'condition', 'odometer', 'cylinders', 'price']]\n",
    "#numeric_df = df[['year', 'odometer', 'price']]\n",
    "X_numeric = numeric_df.drop(columns='price')\n",
    "y_numeric = np.log(numeric_df['price'])\n",
    "X_train_numeric, X_test_numeric, y_train_numeric, y_test_numeric = train_test_split(X_numeric, y_numeric, test_size=0.3, random_state=42) \n",
    "for i in range(1,11):\n",
    "     errors = (get_errors_for_degree_k_model(i, X_train_numeric, y_train_numeric, X_test_numeric, y_test_numeric))\n",
    "     train_mses.append(errors['Train_MSE'])\n",
    "     test_mses.append(errors['Test_MSE'])\n",
    "     print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(train_mses)\n",
    "plt.title(f'The Complexity that minimized Test Error was: {test_mses.index(min(test_mses)) + 1}')\n",
    "plt.suptitle('Simple Cross-Validation with scaling numeric features', fontsize=10, y=0.95)\n",
    "plt.plot(range(1, (length +1)), train_mses, '--o', label = 'training error')\n",
    "plt.plot(range(1, (length + 1)), test_mses, '--o', label = 'testing error')\n",
    "plt.xticks(range(1, (length + 1)), range(1, (length + 1)))\n",
    "plt.xlabel('Degree Complexity')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_df = pd.get_dummies(df, columns=['drive', 'fuel'], drop_first=True, dtype='int64')\n",
    "vehicles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vehicles_df.drop(columns='price')\n",
    "y = np.log(vehicles_df['price'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  \n",
    "selector = SequentialFeatureSelector(LinearRegression(), n_features_to_select=4)\n",
    "best_features = selector.fit_transform(X_train, y_train)\n",
    "best_features_df = pd.DataFrame(vehicles_df, columns = selector.get_feature_names_out())\n",
    "\n",
    "best_features_df['price'] = vehicles_df['price']\n",
    "best_features_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = best_features_df.drop(columns='price')\n",
    "y = np.log(best_features_df['price'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  \n",
    "for i in range(1,11):\n",
    "     errors = (get_errors_for_degree_k_model(i, X_train, y_train, X_test, y_test))\n",
    "     train_mses.append(errors['Train_MSE'])\n",
    "     test_mses.append(errors['Test_MSE'])\n",
    "     print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('poly', PolynomialFeatures()), ('scale', StandardScaler()), ('ridge', Ridge())])\n",
    "param_dict = {\n",
    "    'ridge__alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'poly__degree': [1, 2, 3, 4, 5, 6]\n",
    "}\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = best_features_df.drop(columns='price')\n",
    "y = np.log(best_features_df['price'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, param_grid=param_dict)\n",
    "grid.fit(X_train, y_train)\n",
    "train_preds = grid.predict(X_train)\n",
    "test_preds = grid.predict(X_test)\n",
    "\n",
    "errors = error_metrics(y_train, train_preds, y_test, test_preds)\n",
    "grid_params = grid.best_params_\n",
    "#coefficients = grid_params.named_steps['ridge'].coef_\n",
    "\n",
    "\n",
    "\n",
    "# Answer check\n",
    "print(f'Train MSE: {errors[\"Train_MSE\"]}')\n",
    "print(f'Test MSE: {errors[\"Test_MSE\"]}')\n",
    "print(f'Train R2: {errors[\"Train_R2_Score\"]}')\n",
    "print(f'Test R2: {errors[\"Test_R2_Score\"]}')\n",
    "print(f'Best Degree: {list(grid_params.values())[0]}')\n",
    "print(f'Best Alpha: {list(grid_params.values())[1]}')\n",
    "#print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Analysis:\n",
    "#### Simple Cross Validation Results\n",
    "The table below shows the training and test Mean Squared Error and R2 Scores for the four cross validation models. The model that performed the best was model four which used the parameters determined using sequential feature selection. The test MSE was 0.240613496 and the R2 score was 0.646414436. However, this was only marginally better than the other models\n",
    "\n",
    "#### GridSearchCV with Ridge Regression Results\n",
    "The results from the hyperparameter tuning are shown in the table below. Overall the Ridge Regression performed slightly worse than Simple Cross Validation. With the best test MSE of 0.24615111472551 and R2 score of 0.638527812829079 for hyperparameters alpha = 0.01 and degree = 5. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
